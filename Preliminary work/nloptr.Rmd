---
title: "nloptr"
author: "VAILHERE"
date: "03/06/2019"
output:
  html_document:
    df_print: paged
---

# Package installation

```{r echo=FALSE}
library(nloptr)
```

```{r eval=FALSE}
install.packages("nloptr")
library(nloptr)   #Loading previously installed methods
#Loading must be done at each opening of the Rmardown

```

# Documentation

For more precisions, please consult following websites :  

- https://cran.r-project.org/web/packages/nloptr/vignettes/nloptr.pdf 

- https://cran.r-project.org/web/packages/nloptr/nloptr.pdf

- https://nlopt.readthedocs.io/en/latest/


# List of available algorithms


| MethodName   | MethodType                         | Local_Global |Df_Gb                        |Constraints                       |
| :----------- | :--------------------------------- | :------------|:--------------------------- |:-------------------------------- |
| AGS          | Attributes generalization sequences| G            |N                           | Bounds + inequality constraints  |
| AUGLAG(_EQ)  | Augmented Lagrangian Algorithm     | /            | /                           | Bounds + inequality and equality constraints |
| BOBYQA       | Bound Optimization by Quadratic Approximation             | L            |N     | Bounds                           |
| CCSAQ        | Conservative Convex Separable Approximation Quadratic      | L            |D    | Bounds + inequality constraints (and his Jacobian) |
| COBYLA       | Constrained Optimization by Linear | L            |N                            | Bounds + inequality constraints  |
| CRS_2_LM     | Controlled Random Search           | G            |N                            | Bounds                           |
| DIRECT(_NOSCAL) | Dividing Rectangles Algorithm for Global Optimization (NOSCAL: without rescaling, advised if your dimensions do not have equal weight, e.g. if you have a "long and skinny" search space and your function varies at about the same speed in all directions)| G            |N         | Bounds    |
| DIRECT-L(_RAND)(_NOSCAL)     | DIRECT biased towards local search. This version is "more biased towards local search" so that it is more efficient for functions without too many local minima.(RAND: randomized variant)(NOSCAL: see here-before)| G            |N                            | Bounds     |
| ESCH         | Evolutionnary Algorithm            | G            |N                            | Bounds                           |
| ISRES        | Improved Stochastic Ranking Evolution Strategy            | G            |N     | Bounds + inequality and equality constraints |
| LBFGS        | Low-storage Broyden-Fletcher-Goldfarb-Shanno               | L            |D    | Bounds                           |
| MLSL(_LDS)   | Multi-level Single-linkage (with low-discrepancy sequence | G        |N/D       | Bounds                           |
| MMA          | Method of Moving Asymptotes        | L            |D                            | Bounds + inequality constraints (and his Jacobian) |
| NELDERMEAD   | Nelder-Mead Simplex                | L            |N                            | Bounds                           |
| NEWUOA       | New Unconstrained Optimization with quadratic Approximation| L           |N     | Bounds (only with NEWUOA_BOUND)  |
| PRAXIS       | Principal Axis                     | L            |N                            | /                                |
| SBPLX        | Subplex Algorithm                  | L            |N                            | Bounds                           |
| SLSQP        | Sequential Quadratic Programming   | L            |D                            | Bounds + inequality and equality constraints (and their Jacobian) |
| STOGO(_RAND) | Stochastic Global Optimization (randomized variant)       | G        |D         | Bounds                           |
| TNEWTON(_PRECOND)(_RESTART) | Truncated Newton (with preconditioning)(with restarting)   | L   |D       | Bounds                  |
| VAR1         | Shifted Limited-memory Variable-metric (using rank 2 method)| L           |D    | Bounds                           |
| VAR2         | Shifted Limited-memory Variable-metric (using rank 1 method)| L           |D    | Bounds                           |



( ) -> can be added to the algorithm but is not mandatory

Df_Gb -> Derivativefree_Gradientbased (N -> derivative free ; D -> gradient-based algorithms)

For TNEWTON we can add PRECOND or RESTART or both together (in the order PRECOND_RESTART, the other way doesn't work)

For DIRECT_L we can add RAND or NOSCAL or both together (in the order RAND_NOSCAL, the other way doesn't work)

The variant AUGLAG_EQ only uses penalty functions for equality constraints, inequality constraints are passed through to the subsidiary algorithm to be handled directly; in this case, the subsidiary algorithm must handle inequality constraints.


Examples of algorithm name to use with nloptr : 
NLOPT_LN_COBYLA ; NLOPT_GN_MLSL_LDS ; NLOPT_LD_TNEWTON_PRECOND_RESTART


# Functionalities


| Functionality     | Description                        |
| :-----------      | :--------------------------------- |
|Stopping criterion |- stopval : Stop minimization when an objective value <= stopval is found. Setting stopval to -Inf disables this stopping criterion (default)|
|                   |- ftol_rel : Stop when an optimization step (or an estimate of the optimum) changes the objective function value by less than ftol_rel multiplied by the absolute value of the function value|
|                   |- ftol_abs : Stop when an optimization step (or an estimate of the optimum) changes the objective function value by less than ftol_rel multiplied by the absolute value of the function value|
|                   |- xtol_rel : Stop when an optimization step (or an estimate of the optimum) changes every parameter by less than xtol_rel multiplied by the absolute value of the parameter. Default value:   1.0e-04| 
|                   |- xtol_abs : xtol_abs is a vector of length n (the number of elements in x) giving the tolerances: stop when an optimization step (or an estimate of the optimum) changes every parameter x[i] by less than xtol_abs[i]|
|                   |- maxeval : Stop when the number of function evaluations exceeds maxeval. Default value:   100|
|                   |- maxtime :Stop when the optimization time (in seconds) exceeds maxtime| 
|                   |- tol_constraints_ineq : The parameter tol_constraints_ineq is a vector of tolerances. Each tolerance corresponds to one of the inequality constraints. The tolerance is used for the purpose of stopping criteria only: a point x is considered feasible for judging whether to stop the optimization if eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will try not to consider any x to be converged unless eval_g_ineq(x) is strictly non-positive; generally, at least a small positive tolerance is advisable to reduce sensitivity to rounding errors|
|                   |- tol_constraints_eq : The parameter tol_constraints_eq is a vector of tolerances. Each tolerance corresponds to one of the equality constraints. The tolerance is used for the purpose of stopping criteria only: a point x is considered feasible for judging whether to stop the optimization if abs( eval_g_ineq(x) ) <= tol. For equality constraints, a small positive tolerance is strongly advised in order to allow NLopt to converge even if the equality constraint is slightly nonzero|
|check_derivatives|The option check_derivatives can be activated to compare the user-supplied analytic gradients with finite difference approximations|
|print_level|The option print_level controls how much output is shown during the optimization process. Possible values: 0 (default): no output; 1: show iteration number and value of objective function; 2: 1 + show value of (in)equalities; 3: 2 + show value of controls|
|ranseed|For stochastic optimization algorithms, pseudorandom numbers are generated. Set the random seed using ranseed if you want to use a 'deterministic' sequence of pseudorandom numbers, i.e. the same sequence from run to run|


# Interface 

The user should at least provide a function taking as first argument the vector of estimated parameters and returning the corresponding value of the function to minimize. The name of this first argument can be freely chosen by the user.


The function may have any number of supplementary arguments, e.g.:
f <- function( x, a, b ) { return( (a*x[1] + b)^3 - x[2] ) }


Depending on the method used, the user may have to provide also functions for computing the gradient, equality / inequality constraints and its jacobian (see examples in further sections).



# Examples

## Detailed example

```{r echo=TRUE, eval=TRUE}
## Rosenbrock Banana function
#  The global minimum is 0 and is obtained at (1,1)  
eval_f <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

nloptr( x0=c(0,2.5), eval_f=eval_f, opts = list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1.0e-8,"print_level" = 3, "maxeval"=10))
```

We can see the values of the function and parameters obtained for the first 10 iterations (maxeval=10) thanks to the functionnality print_level = 2. 
The results printed show the number of iterations, equality and inequality constraints prescribed and the current value of objective function and estimated parameters (called controls). The maximum number of iterations prescribed in this example is obviously not enough to reach an estimation close to the expected solution. Setting it to 200 gives better results (see here-after).

**It should be noted that, contrary to the Matlab fminsearch implementation, iterations here are equivalent to function evaluations, and thus an equality or reduction of the function between 2 successive iterations is not guaranted. The minimum may thus not be obtained at the last iteration.**

```{r echo=TRUE, eval=TRUE}
nloptr( x0=c(0,2.5), eval_f=eval_f, opts = list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1.0e-8, "maxeval"=200))
```


## Function with additional arguments 

```{r eval=FALSE}
f <- function( x, a, b ) {
return( (a*x[1] + b)^3 - x[2] )
}

#Define the parameters
a <- c(2,-1)
b <- c(0, 1)
x0<- c(1, 5)

opts <- list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1.0e-8)

nloptr(x0=x0, eval_f=f, opts=opts, a=a, b=b )

```

An example with a function to minimize that requires additional arguments (here a and b).


## Example of gradient-based method

```{r eval=FALSE}
## Rosenbrock Banana function
#  The global minimum is 0 and is obtained at (1,1)  
eval_f <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

eval_grad <- function(x) {
return( c( -400 * x[1]*(x[2] - x[1] * x[1]) + 2*(-1 + x[1]),
           200*(x[2] - x[1] * x[1]) ) )
}

nloptr(x0=c(0,2.5), eval_f=eval_f, eval_grad_f = eval_grad, opts = list("algorithm"="NLOPT_LD_LBFGS"))
```

The function computing the gradient is provided using the eval_grad_f argument.


## Example of using bounds constraints 

```{r eval=FALSE}
# Fletcher and Powell's helic valley
# The global minimum is 0 and is obtained at (1,0,0)  
fphv <- function(x){
100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 + (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2}

x0 <- c(-1, 0, 0)
lower <- c(-Inf, 0, 0)
upper <- c( Inf, 0.5, 1)


nloptr(x0=x0, eval_f=fphv, lb=lower, ub=upper,  opts = list("algorithm"="NLOPT_LN_NELDERMEAD"))

```

They are provided using the lb and ub arguments. -Inf and Inf can be used if the parameter has no lower and/or upper bounds. By default there are no lower and upper bounds for any of the controls.


## Example of using equality constraints

```{r eval=FALSE}
## Rosenbrock Banana function
#  The global minimum is 0 and is obtained at (1,1)  
eval_f0 <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

# constraint function
eval_g0 <- function( x ){
return( c(x[1] - x[2]) )
}

lower <- c(-2, -2)
upper <- c( 2,2)

nloptr(x0=c(0,2), eval_f=eval_f0, eval_g_eq = eval_g0, lb=lower, ub=upper, opts = list("algorithm"="NLOPT_GN_ISRES","xtol_rel"=1.0e-8, "tol_constraints_eq"=1e-6,"maxeval"=500))
```

A function returning a vector of equality constraints must be provided using the eval_g_eq argument.

**Hum, this method is not very efficient ... and seems that providing ub and lb is mandatory**


## Example of inequality constraints

```{r eval=FALSE}
## Rosenbrock Banana function
#  The global minimum is 0 and is obtained at (1,1)  
eval_f <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

hin <- function(x) {
  return(x[1]-x[2]-0.1)
}

nloptr( x0=c(0,2.5), eval_f=eval_f, eval_g_ineq = hin, opts = list("algorithm"="NLOPT_LN_COBYLA", "xtol_rel"=1.0e-10, "maxeval"=300,"tol_constraints_eq"=1e-5))
```

The constraint must be <= 0 (be careful if you use the cobyla function directly, the constraint must be >= 0 in the 2.6.1 version of the package)

**Hum, this method does not seem to work very well ... the result obtained is not close to the expected minima.**

## Example of jacobian

```{r eval=FALSE}

## Solve the Hock-Schittkowski problem no. 100 with analytic gradients
eval_f <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

eval_grad <- function(x) {
return( c( -400 * x[1]*(x[2] - x[1] * x[1]) + 2*(-1 + x[1]),
           200*(x[2] - x[1] * x[1]) ) )
}

hin <- function(x) {
  return(x[1]-x[2]-0.1)
}

hinjac <- function(x) {
  return(c(x[1], -x[2]))
}

nloptr(x0=c(0,2.5), eval_f=eval_f, eval_grad_f=eval_grad, eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = list("algorithm"="NLOPT_LD_MMA", "xtol_rel"=1.0e-8,"maxeval"=300))
```

**Should be noted that a better convergence is obtained without the constraint on this example ...**

## Example of coupling global and local methods

```{r eval=FALSE}
hartmann6 <- function(x) {
n <- length(x)

a <- c(1.0, 1.2, 3.0, 3.2)
A <- matrix(c(10.0, 0.05, 3.0, 17.0, 3.0, 10.0, 3.5, 8.0, 17.0, 17.0, 1.7, 0.05, 3.5, 0.1, 10.0, 10.0, 1.7, 8.0, 17.0, 0.1, 8.0, 14.0, 8.0, 14.0), nrow=4, ncol=6)

B <- matrix(c(.1312,.2329,.2348,.4047, .1696,.4135,.1451,.8828, .5569,.8307,.3522,.8732, .0124,.3736,.2883,.5743, .8283,.1004,.3047,.1091, .5886,.9991,.6650,.0381), nrow=4, ncol=6)

fun <- 0.0

for (i in 1:4) {
fun <- fun - a[i] * exp(-sum(A[i,]*(x-B[i,])^2))
}
return(fun)
}

local_options=list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1e-8, "maxeval"=1000)

options=list("algorithm"="NLOPT_GN_MLSL", "xtol_rel"=1e-8, "maxeval"=1000, "local_opts"=local_options)

nloptr(x0=rep(0,6),eval_f=hartmann6, opts = options, lb=rep(0,6), ub=rep(1,6))
```

**Be careful to use a local method compatible with the global one (e.g. derivative free or gradient-based)**



