---
title: "nloptr"
author: "VAILHERE"
date: "03/06/2019"
output: word_document
---

# Package installation

```{r echo=FALSE}
library(nloptr)
```

```{r eval=FALSE}
install.packages("nloptr")
library(nloptr)   #Loading previously installed methods
#Loading must be done at each opening of the Rmardown

```


# List of available algorithms

## Documentation

For more precisions, please consult following websites :  

- https://cran.r-project.org/web/packages/nloptr/vignettes/nloptr.pdf 

- https://cran.r-project.org/web/packages/nloptr/nloptr.pdf

- https://nlopt.readthedocs.io/en/latest/


## Board


| MethodName   | MethodType                         | Local_Global |Df_Gb                        |Constraints                       |
| :----------- | :--------------------------------- | :------------|:--------------------------- |:-------------------------------- |
| AGS          | Attributes’ generalization sequences| G            |N                           | Bounds + inequality constraints  |
| AUGLAG(_EQ)  | Augmented Lagrangian Algorithm     | /            | /                           | Bounds + inequality and equality constraints |
| BOBYQA       | Bound Optimization by Quadratic Approximation             | L            |N     | Bounds                           |
| CCSAQ        | Conservative Convex Separable Approximation Quadratic      | L            |D    | Bounds + inequality constraints (and his Jacobian) |
| COBYLA       | Constrained Optimization by Linear | L            |N                            | Bounds + inequality constraints  |
| CRS_2_LM     | Controlled Random Search           | G            |N                            | Bounds                           |
| DIRECT(_NOSCAL) | Dividing Rectangles Algorithm for Global Optimization (NOSCAL: without rescaling, advised if your dimensions do not have equal weight, e.g. if you have a "long and skinny" search space and your function varies at about the same speed in all directions)| G            |N         | Bounds    |
| DIRECT-L(_RAND)(_NOSCAL)     | DIRECT biased towards local search. This version is "more biased towards local search" so that it is more efficient for functions without too many local minima.(RAND: randomized variant)(NOSCAL: see here-before)| G            |N                            | Bounds     |
| ESCH         | Evolutionnary Algorithm            | G            |N                            | Bounds                           |
| ISRES        | Improved Stochastic Ranking Evolution Strategy            | G            |N     | Bounds + inequality and equality constraints |
| LBFGS        | Low-storage Broyden-Fletcher-Goldfarb-Shanno               | L            |D    | Bounds                           |
| MLSL(_LDS)   | Multi-level Single-linkage (with low-discrepancy sequence | G        |N/D       | Bounds                           |
| MMA          | Method of Moving Asymptotes        | L            |D                            | Bounds + inequality constraints (and his Jacobian) |
| NELDERMEAD   | Nelder-Mead Simplex                | L            |N                            | Bounds                           |
| NEWUOA       | New Unconstrained Optimization with quadratic Approximation| L           |N     | Bounds (only with NEWUOA_BOUND)  |
| PRAXIS       | Principal Axis                     | L            |N                            | /                                |
| SBPLX        | Subplex Algorithm                  | L            |N                            | Bounds                           |
| SLSQP        | Sequential Quadratic Programming   | L            |D                            | Bounds + inequality and equality constraints (and their Jacobian) |
| STOGO(_RAND) | Stochastic Global Optimization (randomized variant)       | G        |D         | Bounds                           |
| TNEWTON(_PRECOND)(_RESTART) | Truncated Newton (with preconditioning)(with restarting)   | L   |D       | Bounds                  |
| VAR1         | Shifted Limited-memory Variable-metric (using rank 2 method)| L           |D    | Bounds                           |
| VAR2         | Shifted Limited-memory Variable-metric (using rank 1 method)| L           |D    | Bounds                           |



( ) -> can be added to the algorithm but is not mandatory

Df_Gb -> Derivativefree_Gradientbased (N -> derivative free ; D -> gradient-based algorithms)

For TNEWTON we can add PRECOND or RESTART or both together (in the order PRECOND_RESTART, the other way doesn't work)

For DIRECT_L we can add RAND or NOSCAL or both together (in the order RAND_NOSCAL, the other way doesn't work)

The variant AUGLAG_EQ only uses penalty functions for equality constraints, inequality constraints are passed through to the subsidiary algorithm to be handled directly; in this case, the subsidiary algorithm must handle inequality constraints.


## Examples of algorithm name to use with nloptr : 
NLOPT_LN_COBYLA ; NLOPT_GN_MLSL_LDS ; NLOPT_LD_TNEWTON_PRECOND_RESTART


# Differents functionality


| Functionality     | Description                        |
| :-----------      | :--------------------------------- |
|Stopping criterion |- stopval : Stop minimization when an objective value <= stopval is found. Setting stopval to -Inf disables this stopping criterion (default)|
|                   |- ftol_rel : Stop when an optimization step (or an estimate of the optimum) changes the objective function value by less than ftol_rel multiplied by the absolute value of the function value|
|                   |- ftol_abs : Stop when an optimization step (or an estimate of the optimum) changes the objective function value by less than ftol_rel multiplied by the absolute value of the function value|
|                   |- xtol_rel : Stop when an optimization step (or an estimate of the optimum) changes every parameter by less than xtol_rel multiplied by the absolute value of the parameter. Default value:   1.0e-04| 
|                   |- xtol_abs : xtol_abs is a vector of length n (the number of elements in x) giving the tolerances: stop when an optimization step (or an estimate of the optimum) changes every parameter x[i] by less than xtol_abs[i]|
|                   |- maxeval : Stop when the number of function evaluations exceeds maxeval. Default value:   100|
|                   |- maxtime :Stop when the optimization time (in seconds) exceeds maxtime| 
|                   |- tol_constraints_ineq : The parameter tol_constraints_ineq is a vector of tolerances. Each tolerance corresponds to one of the inequality constraints. The tolerance is used for the purpose of stopping criteria only: a point x is considered feasible for judging whether to stop the optimization if eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will try not to consider any x to be converged unless eval_g_ineq(x) is strictly non-positive; generally, at least a small positive tolerance is advisable to reduce sensitivity to rounding errors|
|                   |- tol_constraints_eq : The parameter tol_constraints_eq is a vector of tolerances. Each tolerance corresponds to one of the equality constraints. The tolerance is used for the purpose of stopping criteria only: a point x is considered feasible for judging whether to stop the optimization if abs( eval_g_ineq(x) ) <= tol. For equality constraints, a small positive tolerance is strongly advised in order to allow NLopt to converge even if the equality constraint is slightly nonzero|
|check_derivatives|The option check_derivatives can be activated to compare the user-supplied analytic gradients with finite difference approximations|
|print_level|The option print_level controls how much output is shown during the optimization process. Possible values: 0 (default): no output; 1: show iteration number and value of objective function; 2: 1 + show value of (in)equalities; 3: 2 + show value of controls|
|ranseed|For stochastic optimization algorithms, pseudorandom numbers are generated. Set the random seed using ranseed if you want to use a 'deterministic' sequence of pseudorandom numbers, i.e. the same sequence from run to run|


nloptr( x0=c(1.234,5.678), eval_f=eval_f0, lb = c(-Inf,0), ub = c(Inf,Inf), eval_g_ineq = eval_g0, opts = list("algorithm"="NLOPT_LN_COBYLA", "xtol_rel"=1.0e-8,"print_level" = 2, "check_derivatives" = TRUE, "maxtime" = 3), a = a, b = b )

# Interface 

The user should at least provide a function taking as first argument the vector of estimated parameters and returning the corresponding value of the function to minimize. The name of this first argument can be freely chosen by the user.


The function may have any number of supplementary arguments, e.g.:
f <- function( x, a, b ) { return( (a*x[1] + b)^3 - x[2] ) }


Depending on the method used, the user may have to provide also functions for computing the gradient, equality / inequality constraints and its jacobian (see examples in further sections).



# Example

## Detailed example

```{r echo=TRUE, eval=TRUE}
## Rosenbrock Banana function
eval_f <- function(x) {
return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

nloptr( x0=c(-1.2,1), eval_f=eval_f, opts = list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1.0e-8,"print_level" = 2, "maxeval"=10))
```

We can see the first 10 iterationsn (maxeval=10) thanks to the functionnality print_level = 2.
For the results, there is the number of itérations, equality constraints and inequality constraints and the current value of objective function. 



## Function with additional arguments 

```{r eval=FALSE}
f <- function( x, a, b ) {
return( (a*x[1] + b)^3 - x[2] )
}

#Define the parameters
a <- c(2,-1)
b <- c(0, 1)
x0<- c(1, 5)

opts <- list("algorithm"="NLOPT_LN_NELDERMEAD", "xtol_rel"=1.0e-8)

nloptr(x0=x0, eval_f=f, opts=opts, a=a, b=b )

```


## Example of derivatife-free method

```{r eval=FALSE}
fr <- function(x) { ## Rosenbrock Banana function
100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2
}

nloptr(x0=c(0,0,0), eval_f=fr, opts = list("algorithm"="NLOPT_LN_BOBYQA"))

```


## Example of gradient-based method

```{r eval=FALSE}
x0 <- c(1, 2, 0, 4, 0, 1, 1)

fn <- function(x) {
(x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}

gr <- function(x) {
c( 2 * x[1] - 20,
10 * x[2] - 120,
4 * x[3]^3,
6 * x[4] - 66,
60 * x[5]^5,
14 * x[6] - 4 * x[7] - 10,
4 * x[7]^3 - 4 * x[6] - 8 )}

nloptr(x0=x0, eval_f=fn, eval_grad_f = gr, opts = list("algorithm"="NLOPT_LD_LBFGS"))
```


## Example of bounds constraints 

```{r eval=FALSE}
# Fletcher and Powell's helic valley
fphv <- function(x){
100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 + (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2}

x0 <- c(-1, 0, 0)
lower <- c(-Inf, 0, 0)
upper <- c( Inf, 0.5, 1)


nloptr(x0=x0, eval_f=fphv, lb=lower, ub=upper,  opts = list("algorithm"="NLOPT_LN_NELDERMEAD"))

```

By default there are no lower and upper bounds for any of the controls.


## Example of equality constraints

```{r eval=FALSE}
# objective function
eval_f0 <- function( x ){
return( sqrt(x[2]) )
}

# constraint function
eval_g0 <- function( x ){
return( x[1] - x[2] )
}

nloptr(x0=c(2,3), eval_f=eval_f0, eval_g_eq = eval_g0, opts = list("algorithm"="NLOPT_GN_ISRES"))
```


## Example of inequality constraints


```{r eval=FALSE}
x0 <- c(1, 2, 0, 4, 0, 1, 1)

fn <- function(x) {
(x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}

hin <- function(x) {
h <- numeric(4)
h[1] <- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
h[2] <- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
h[3] <- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
h[4] <- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6] +11*x[7]
return(h)
}

nloptr( x0=x0, eval_f=fn, eval_g_ineq = hin, opts = list("algorithm"="NLOPT_LN_COBYLA"))
```


The constraint must be <= 0 (be careful if you use the cobyla function directly, the constraint must be >= 0 in the 2.6.1 version of the package)


## Example of jacobian

```{r eval=FALSE}

## Solve the Hock-Schittkowski problem no. 100 with analytic gradients
x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1)

fn.hs100 <- function(x) {
(x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}

hin.hs100 <- function(x) {
h <- numeric(4)
h[1] <- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
h[2] <- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
h[3] <- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
h[4] <- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6] +11*x[7]
return(h)
}

gr.hs100 <- function(x) {
c( 2 * x[1] - 20,
10 * x[2] - 120,
4 * x[3]^3,
6 * x[4] - 66,
60 * x[5]^5,
14 * x[6] - 4 * x[7] - 10,
4 * x[7]^3 - 4 * x[6] - 8 )}

hinjac.hs100 <- function(x) {
matrix(c(4*x[1], 12*x[2]^3, 1, 8*x[4], 5, 0, 0,
7, 3, 20*x[3], 1, -1, 0, 0,
23, 2*x[2], 0, 0, 0, 12*x[6], -8,
8*x[1]-3*x[2], 2*x[2]-3*x[1], 4*x[3], 0, 0, 5, -11), 4, 7, byrow=TRUE)
}

nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f=gr.hs100, eval_g_ineq = hin.hs100, eval_jac_g_ineq = hinjac.hs100, opts = list("algorithm"="NLOPT_LD_MMA"))

```


## Example of coupling global and local methods


```{r eval=FALSE}
hartmann6 <- function(x) {
n <- length(x)

a <- c(1.0, 1.2, 3.0, 3.2)
A <- matrix(c(10.0, 0.05, 3.0, 17.0, 3.0, 10.0, 3.5, 8.0, 17.0, 17.0, 1.7, 0.05, 3.5, 0.1, 10.0, 10.0, 1.7, 8.0, 17.0, 0.1, 8.0, 14.0, 8.0, 14.0), nrow=4, ncol=6)

B <- matrix(c(.1312,.2329,.2348,.4047, .1696,.4135,.1451,.8828, .5569,.8307,.3522,.8732, .0124,.3736,.2883,.5743, .8283,.1004,.3047,.1091, .5886,.9991,.6650,.0381), nrow=4, ncol=6)

fun <- 0.0

for (i in 1:4) {
fun <- fun - a[i] * exp(-sum(A[i,]*(x-B[i,])^2))
}
return(fun)
}

local_opts=list("algorithm"="NLOPT_LD_LBFGS", "xtol_rel"=1e-8, "maxeval"=1000)

opts=list("algorithm"="NLOPT_GN_MLSL", "xtol_rel"=1e-8, "maxeval"=1000, "local_opts"=local_opts)

nloptr(x0=rep(0,6),eval_f=hartmann6, lb=rep(0,6), ub=rep(1,6),
       opts = opts)


```

A retester cela ne marche pas !




## Format de la méthode "nloptr" selon les algorithmes


Nécéssite seulement x0 et la fonction à minimiser -> NELDERMEAD, BOBYQA, DIRECT, DIRECT_L, NEWUOA, SBPLX.

Nécessite comme ci-dessus + le gradient de la fonction -> LBFGS, STOGO, VAR1, VAR2

Nécessite comme ci-dessus + peut prendre la contrainte d'inégalité et son jacobian (indissociable l'un de l'autre) -> SLSQP

Fonctions à tester de nouveau -> COBYLA, MMA (Pas le même résultat que la fonction associé); MLSL (Nécéssite un local optimizer), ISRES (Donne de nouveaux résultats à chaque fois qu'elle est relancé), TNEWTON (Pas le même nombre d'itérations que la fonction associé)


## Brouillon (temporaire sera retiré à la fin)


COBYLA -> nloptr( x0=x0.hs100, eval_f=fn.hs100, eval_g_ineq = hin.hs100, opts = list("algorithm"="NLOPT_LN_COBYLA", "xtol_rel"=1.0e-8, "maxeval" = 2000))  #Donne pas le même résultat que la fonction "cobyla"... #Besoin de x0, la fonction à minimiser et la contrainte d'ingéalité.

NELDERMEAD -> nloptr(x0, fphv, opts = list("algorithm"="NLOPT_LN_NELDERMEAD", "maxeval"=1000)) #Besoin de x0 et et de la fonction à minimiser

GN_MLSL nécéssite un local optimizer et GD_MLSL nécessite le gradient et un local optimizer de la fonction à minimiser. OBligatoire d'avoir un lower et un upper. 

BOBYQA -> nloptr(x0=c(0,0,0), eval_f=fr, lb=c(0,0,0), ub=c(0.5,0.5,0.5), opts = list("algorithm"="NLOPT_LN_BOBYQA", "xtol_rel"=1e-06)) #Besoin de x0 et et de la fonction à minimiser

DIRECT -> nloptr(x0=rep(0,6),eval_f=hartmann6, lb=rep(0,6),ub=rep(1,6),opts = list("algorithm"="NLOPT_GN_DIRECT", "xtol_rel"=1e-8, "maxeval"=1000)) #Besoin de x0 et et de la fonction à minimiser
DIRECT_L -> Même chose que DIRECT

ISRES -> nloptr(x0=x0, eval_f=fn, lb=lb, ub=ub, opts = list("algorithm"="NLOPT_GN_ISRES", "xtol_rel"=1e-6, "maxeval"=10000)) #Besoin de x0 et et de la fonction à minimiser #Donne des valeurs différentes à chaque fois, il doit avoir quelque chose d'aléatoire dans cet algorithme.

LBFGS -> nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f = gr.hs100, opts = list("algorithm"="NLOPT_LD_LBFGS", "xtol_rel"=1e-8)) #nécéssite le gradient de la fonction à minimiser

MMA -> nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f=gr.hs100, eval_g_ineq = hin.hs100, eval_jac_g_ineq = hinjac.hs100, opts = list("algorithm"="NLOPT_LD_MMA", "xtol_rel"=1e-8, "maxeval"=1000)) #Donne pas le même résultat que la fonction mma.... #Besoin du gradient et peut prendre la contrainte d'inégalité et son jacobian (indissociable l'un de l'autre)

NEWUOA -> nloptr(x0=c(1,2),fr, opts = list("algorithm"="NLOPT_LN_NEWUOA", "xtol_rel"=1e-6)) #Besoin de x0 et et de la fonction à minimiser

SBPLX -> nloptr(x0=x0, eval_f=psf, opts = list("algorithm"="NLOPT_LN_SBPLX", "maxeval"=Inf, "ftol_rel"=1e-6)) #Besoin de x0 et et de la fonction à minimiser

SLSQP -> nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f = gr.hs100, opts=list("algorithm"="NLOPT_LD_SLSQP", "xtol_rel"=1e-8)) #nécéssite le gradient de la fonction à minimiser et peut prendre la contrainte d'inégalité et son jacobian (indissociable l'un de l'autre)

STOGO -> nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f = gr.hs100, opts=list("algorithm"="NLOPT_GD_STOGO")) #nécéssite le gradient de la fonction à minimiser

TNEWTON -> nloptr(x0=x0.hs100, eval_f = fn.hs100, eval_grad_f = gr.hs100, opts = list("algorithm"="NLOPT_LD_TNEWTON","xtol_rel"=1e-8)) #nécéssite le gradient de la fonction à minimiser  #Même résultat mais pas le même nombre d'itérations.

VAR1 -> nloptr(x0=x0.hs100, eval_f=fn.hs100, eval_grad_f = gr.hs100, opts=list("algorithm"="NLOPT_LD_VAR1", "xtol_rel"=1e-8))  #nécéssite le gradient de la fonction à minimise   #PAreil pour VAR2


Tester GD_MLSL grâce aux valeurs de MMA


